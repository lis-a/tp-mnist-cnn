{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13585d78-9eea-4ddd-a2a3-e2687f56cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3315be5-6aa1-47c8-bfdd-c07715e1882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba60f10-0e2d-4b33-8af9-3a9b0602cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©finition des transformations\n",
    "tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a53cb5-36ce-48c0-aeb7-8f801bd28a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"../data/raw\", train=True, download=True, transform=tf),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"../data/raw\", train=False, download=True, transform=tf),\n",
    "    batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d180bf38-2457-4f53-a04c-a7271fb9b99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage des 5 premiÃ¨res images\n",
    "batch = next(iter(train_loader))\n",
    "x = batch[0][:5]\n",
    "y = batch[1][:5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(x[i][0], cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label: {y[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c21adf-8b62-4c40-ab87-84b6c9a963b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, n_kernels, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, n_kernels, kernel_size=5),   # 28x28 -> 24x24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                          # 24x24 -> 12x12\n",
    "            nn.Conv2d(n_kernels, n_kernels, kernel_size=5),  # 12x12 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                          # 8x8 -> 4x4\n",
    "            nn.Flatten(),                             # -> n_kernels * 4 * 4\n",
    "            nn.Linear(n_kernels * 4 * 4, 50),\n",
    "            nn.Linear(50, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54a9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramÃ¨tres : 6.422K\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28\n",
    "n_kernels = 6\n",
    "output_size = 10\n",
    "\n",
    "model = ConvNet(input_size, n_kernels, output_size).to(device)\n",
    "print(f\"Nombre de paramÃ¨tres : {sum(p.numel() for p in model.parameters())/1e3:.3f}K\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f204c105-b898-4b92-9734-d20ab9138f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, perm=torch.arange(0, 784).long(), n_epoch=1):\n",
    "    model.train()  # met le modÃ¨le en mode entraÃ®nement\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for step, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Appliquer permutation circulaire si besoin\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"epoch={epoch}, step={step}: train loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18cca10d-7534-4175-b221-34c196085155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, step=0: train loss=2.3177\n",
      "epoch=0, step=100: train loss=0.2790\n",
      "epoch=0, step=200: train loss=0.3607\n",
      "epoch=0, step=300: train loss=0.4194\n",
      "epoch=0, step=400: train loss=0.2407\n",
      "epoch=0, step=500: train loss=0.0702\n",
      "epoch=0, step=600: train loss=0.0878\n",
      "epoch=0, step=700: train loss=0.0845\n",
      "epoch=0, step=800: train loss=0.2575\n",
      "epoch=0, step=900: train loss=0.2101\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ac2ac3-08c4-4347-8345-da07ab81953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()  # mode Ã©valuation\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Appliquer la permutation\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "            logits = model(data)\n",
    "            test_loss += F.cross_entropy(logits, target, reduction='sum').item()\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"test loss={test_loss:.4f}, accuracy={accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5a3b4d-4ffb-41a6-94e4-87610d1c9409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss=0.0994, accuracy=0.9693\n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc778e98-f69a-4234-87a7-077028730bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    input_size = 28 * 28\n",
    "    output_size = 10\n",
    "    n_kernels = 6\n",
    "\n",
    "    model = ConvNet(input_size, n_kernels, output_size).to(device)\n",
    "    print(f\"Parameters={sum(p.numel() for p in model.parameters())/1e3:.3f}K\")\n",
    "\n",
    "    train(model)\n",
    "    test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7fb69c-fecc-4489-b5eb-1ff04232b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters=6.422K\n",
      "epoch=0, step=0: train loss=2.3314\n",
      "epoch=0, step=100: train loss=0.3924\n",
      "epoch=0, step=200: train loss=0.4221\n",
      "epoch=0, step=300: train loss=0.2222\n",
      "epoch=0, step=400: train loss=0.1408\n",
      "epoch=0, step=500: train loss=0.2932\n",
      "epoch=0, step=600: train loss=0.2213\n",
      "epoch=0, step=700: train loss=0.0491\n",
      "epoch=0, step=800: train loss=0.0966\n",
      "epoch=0, step=900: train loss=0.1820\n",
      "test loss=0.1004, accuracy=0.9693\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00ec105-d482-4aca-82bc-766c636b7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),  # aplatissement des images\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a256a317-4bfc-4319-bf40-e2908be11503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters=6.442K\n",
      "epoch=0, step=0: train loss=2.3548\n",
      "epoch=0, step=100: train loss=1.3719\n",
      "epoch=0, step=200: train loss=0.6701\n",
      "epoch=0, step=300: train loss=0.6226\n",
      "epoch=0, step=400: train loss=0.6928\n",
      "epoch=0, step=500: train loss=0.6893\n",
      "epoch=0, step=600: train loss=0.5063\n",
      "epoch=0, step=700: train loss=0.3925\n",
      "epoch=0, step=800: train loss=0.2446\n",
      "epoch=0, step=900: train loss=0.4941\n",
      "test loss=0.4430, accuracy=0.8722\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "n_hidden = 8\n",
    "\n",
    "mlp = MLP(input_size, n_hidden, output_size).to(device)\n",
    "print(f\"Parameters={sum(p.numel() for p in mlp.parameters())/1e3:.3f}K\")\n",
    "\n",
    "train(mlp)\n",
    "test(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b76637-d9db-4dd1-8391-e5b74cbc7ed6",
   "metadata": {},
   "source": [
    "### ð Comparaison CNN vs MLP sur MNIST\n",
    "\n",
    "| CritÃ¨re              | CNN (~6.42K params) | MLP (~6.44K params) |\n",
    "|----------------------|---------------------|----------------------|\n",
    "| **Perte (test)**     | 0.0994              | 0.4430               |\n",
    "| **PrÃ©cision (test)** | 96.93â¯%             | 87.22â¯%              |\n",
    "\n",
    "---\n",
    "\n",
    "### ð§  Analyse simple\n",
    "\n",
    "- Le **CNN** est **plus efficace** que le **MLP** pour reconnaÃ®tre des chiffres sur des images.\n",
    "- Cela s'explique par le fait que :\n",
    "  - Le **CNN** peut repÃ©rer des formes locales (traits, coins, courbes) car il tient compte de la structure de lâimage (grÃ¢ce aux couches de convolution et de pooling).\n",
    "  - Le **MLP**, lui, traite chaque pixel sÃ©parÃ©ment, sans comprendre quâun pixel est proche dâun autre. Il apprend donc moins bien sur des images.\n",
    "\n",
    "---\n",
    "\n",
    "### â Conclusion\n",
    "\n",
    "MÃªme avec presque le **mÃªme nombre de paramÃ¨tres**, le **CNN est beaucoup plus performant** que le MLP sur ce type de donnÃ©es.\n",
    "\n",
    "Il est donc **mieux adaptÃ© pour analyser des images**, car il sait capter la **forme et la position des motifs**, ce que ne fait pas un MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4936ce-b318-474a-b9eb-e812a801501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48ec975-7202-47a6-858a-aa7ff4554664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN permutÃ©] Parameters=6.422K\n",
      "epoch=0, step=0: train loss=2.3049\n",
      "epoch=0, step=100: train loss=1.2912\n",
      "epoch=0, step=200: train loss=0.6717\n",
      "epoch=0, step=300: train loss=0.5261\n",
      "epoch=0, step=400: train loss=0.7551\n",
      "epoch=0, step=500: train loss=0.5791\n",
      "epoch=0, step=600: train loss=0.3367\n",
      "epoch=0, step=700: train loss=0.5788\n",
      "epoch=0, step=800: train loss=0.5605\n",
      "epoch=0, step=900: train loss=0.5001\n",
      "test loss=0.3948, accuracy=0.8805\n"
     ]
    }
   ],
   "source": [
    "convnet_perm = ConvNet(input_size, n_kernels, output_size).to(device)\n",
    "print(f\"[CNN permutÃ©] Parameters={sum(p.numel() for p in convnet_perm.parameters())/1e3:.3f}K\")\n",
    "\n",
    "train(convnet_perm, perm=perm)\n",
    "test(convnet_perm, perm=perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef3d32f0-44c7-4acb-8142-4aa2be614aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP permutÃ©] Parameters=6.442K\n",
      "epoch=0, step=0: train loss=2.3965\n",
      "epoch=0, step=100: train loss=1.4215\n",
      "epoch=0, step=200: train loss=0.9672\n",
      "epoch=0, step=300: train loss=0.7979\n",
      "epoch=0, step=400: train loss=0.5457\n",
      "epoch=0, step=500: train loss=0.7244\n",
      "epoch=0, step=600: train loss=0.6537\n",
      "epoch=0, step=700: train loss=0.7273\n",
      "epoch=0, step=800: train loss=0.5054\n",
      "epoch=0, step=900: train loss=0.5484\n",
      "test loss=0.4516, accuracy=0.8692\n"
     ]
    }
   ],
   "source": [
    "mlp_perm = MLP(input_size, n_hidden, output_size).to(device)\n",
    "print(f\"[MLP permutÃ©] Parameters={sum(p.numel() for p in mlp_perm.parameters())/1e3:.3f}K\")\n",
    "\n",
    "train(mlp_perm, perm=perm)\n",
    "test(mlp_perm, perm=perm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2c127-1036-4a19-8aad-3058fc86114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ð Effet d'une permutation alÃ©atoire sur les performances\n",
    "\n",
    "> Nous avons appliquÃ© une permutation alÃ©atoire des pixels sur les images MNIST, et rÃ©entraÃ®nÃ© les deux modÃ¨les (CNN & MLP).\n",
    "\n",
    "#### ð RÃ©sultats comparÃ©s\n",
    "\n",
    "| ModÃ¨le        | Accuracy sans permutation | Accuracy avec permutation |\n",
    "|---------------|---------------------------|----------------------------|\n",
    "| **CNN**       | 96.93â¯%                   | 88.05â¯%                   |\n",
    "| **MLP**       | 87.22â¯%                   | 86.92â¯%                   |\n",
    "\n",
    "#### ð§  Analyse\n",
    "\n",
    "- Le **CNN** perd **prÃ¨s de 9 points** dâaccuracy, car la permutation **dÃ©truit la structure spatiale** des images.\n",
    "- Le **MLP**, en revanche, nâest **presque pas affectÃ©**, car il **nâexploite pas la disposition spatiale des pixels**.\n",
    "- Cela confirme que les **CNN sont trÃ¨s dÃ©pendants de la structure locale** des images (ce qui est une force quand elle est prÃ©sente), alors que les **MLP traitent chaque pixel indÃ©pendamment**.\n",
    "\n",
    "#### â Conclusion\n",
    "\n",
    "- La permutation des pixels permet de montrer que le **CNN utilise rÃ©ellement la structure spatiale** pour apprendre.\n",
    "- Le **MLP**, lui, est dÃ©jÃ  insensible Ã  cette structure, ce qui explique pourquoi sa performance reste stableâ¦ mais infÃ©rieure Ã  celle dâun CNN non permutÃ©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c1db47-7b83-4607-82dd-f78ac9157e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modÃ¨le CNN original (non permutÃ©)\n",
    "torch.save(model.state_dict(), \"../model/mnist-0.0.1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543ab3c-a735-4922-ab85-04b2c185567a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
